# Building Clues for StudentEval Prompts

## Procedure for doing a new problem

1. Read clues.yaml to see the autogenerated clues. Also look at [problems.yaml].
   You will want to modify them.
2. Create a prompt template for the problem called `prompt_$PROBLEM.yaml`. Use
   one of the existing templates, and modify the last message to include
   the clues for PROBLEM. See `prompt_add_word.yaml` for an example. *Notice
   how the clues are formatted a little differently than the autogenerated
   clues.
3. Extract the edges:
   
   ```
   python3 edges.py extract --graph ../tagging_clues/tagging_graphs_sep27/$PROBLEM.yaml --annotations edges_$PROBLEM.yaml
   ```
4. Label the edges:

   ```
   python3 main.py --data edges_$PROBLEM.yaml --prompt-template prompt_$PROBLEM.yaml       
   ```

   **Check all labels and modified.**

5. Extract the start nodes (not nodes omg):

    ```
    python3 first_attempts.py extract --graph ../tagging_clues/tagging_graphs_sep27/$PROBLEM.yaml --first-attempts first_attempts_$PROBLEM.yaml   
    ```

6. Label the start nodes (manually)

7. Patch back the edits to the YAML file:

    ```
    python3 edges.py patchback --graph ../tagging_clues/tagging_graphs_sep27/$PROBLEM.yaml --first-attempts first_attempts_$PROBLEM.yaml
    python3 edges.py patchback --graph ../tagging_clues/tagging_graphs_sep27/$PROBLEM.yaml --annotations edges_$PROBLEM.yaml
    ```
    
   



## How to use an LLM to determine the clues for a problem.

1. **Build `success_prompts.yaml`**:  
   Subset the StudentEval dataset to only successful prompts. Group by problem.  
   Format each problem group as:

   ```
   prompt_1 \n \n \n ... \n \n prompt_n
   ```

   Save all to the YAML file coupled with the problem name.

2. **Create `template.yaml`**:
   - role: system  
     ...
   - ... few shot examples ...  
   - role: user  
     content: |
     The following prompts all describe the same function:  
     $prompts  
     List the concepts that are common to the majority of the prompts above.


## Annotating Edges

I am using an LLM for data annotation and manually checking/modifying the
results. The results go in a file that looks like this:

```yaml
- template_key1: string
  ...
  template_keyN: string
  _response: string
  _status: "generated" | "accepted" | "modified" | "rejected"
```

On first run, everything is "generated". On a re-run, only rejected is
regenerated. Annotation completes when everything is accepted or rejected.

The prompt template is a YAML file looks like a conversation but uses the
template keys (not _response or _status). For example:

```yaml
- role: system
  content: You are an AI assistant that helps with data annotation.
- role: user
  content: What is the capital of France?
- role: assistant
  content: Paris.
- role: user
  content: What is the capital of $LOCATION?
```


[problems.yaml]: https://github.com/Wellesley-EASEL-lab/StudentEval/blob/main/problems.yaml